---
title: "R Notebook"
output:
  html_document:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 62
---
```{r}
options(repos = c(CRAN = "https://cran.rstudio.com"))
```

```{r}
install.packages("readxl")
```


 
| Name           | ID         |
|----------------|------------|
| Taif alrubeaan |  442202301 |
| Raghad Alboqami|  442200455 |
| Leena Alzahrani|  443200847 |
| Sarah Alghanim |  442201747 |
 
  
  

-----------------------------------
The goal of collecting this dataset:
-----------------------------------
our goal is to Identify whether the chances of of a heart attack occurrence is high or low for the patient

-----------------------------------
#classification and clustering goal:
------------------------------------

Classification: We will classify target people into different groups based on appearance of muilty symptoms of Heart attack disease .This will allow us to predict the people who target to have heart attack based on certain factors .

Clustering: We will create a set of clusters for people who have similar symptoms. This will allow us to identify groups of people who are have the similar symptoms and target to have heart attack .

We believe that by using these data mining tasks, we can build a model that can accurately predict the people target to have heart attack.
---------
1 Problem
---------
Recently, Cardiovascular diseases rate has been increased, which leads death globally. Early detection of the symptoms is important to prevent the disease from fully developing and reduce premature mortality. In our project, we will study and analyze patients’ data that will help us well in identifying symptoms that you don't recognize as a sign of a heart disease and help many people to take precautions by predicting the possibility of having a Heart attack.

------------------
2 Data Mining Task
------------------
 In our project we will use two data mining tasks to help us predict the possibility of having cardiovascular diseases which are classification and clustering. For classification we will train our model to be able to classify if the patient has cardiovascular diseases or not using (target) class based on a set of medical examinations like blood pressure, serum cholesterol , ST depression, etc. For the clustering our model will create a set of clusters for the patient who have similar characteristics, then these clusters will be used to predict new patients’ results.
 


3 Data 
The source :
 https://www.kaggle.com/datasets/andrewmvd/ocular-disease-recognition-odir5k
Number of objects: 303
 Number of attributes: 14
----------------------------
 Attribute characteristics:
---------------------------
| Attribute Name | Data Type | Description                                            | Possible Values     |                                                      
|----------------|-----------|--------------------------------------------------------|---------------------|
| age            | Numeric   | Age (in years)                                          | Range between 29-77 |                                                      |                |           |                                                        |                     |
| sex            | Binary    | Gender                                                  | 1 = Male; 0 = Female |                                                     |               |           |                                                          |                     |                       
| cp             | Numeric   | Chest Pain Type                                         | 1: Typical Angina, 2: Atypical Angina, 3: Non-Anginal Pain, 4: Asymptomatic |  |           |                                                        |                            | 
| trestbps       | Numeric   | Resting Blood Pressure                                  | Range between 94-200 |                                                     |                  |        |                                                           |                      |
| chol           | Numeric   | Serum Cholesterol in mg/dL                              | Range between 126-564                                                   
| fbs            | Binary    | Fasting Blood Sugar > 120 mg/dL (likely to be diabetic)  | 1 = True, 0 = False                                                       |                |         |                                                           |                         | 
| Restecg        | Binary    | Resting Electrocardiogram                               | 0 = Normal, 1 = Abnormal                                                  |                 |          |                                                          |                           |
| Oldpeak        | Numeric   | ST Depression                                           | Range between 0-6.20                                                      |                |           |                                                          |                             |
| thalach        | Numeric   | Heart Rate Achieved                                     | Range between 71-202  |                                                    |               |           |                                                         |                        |
| exang          | Binary    | Exercise Induced Angina                                 | 1 = Yes; 0 = No                                                           |                |          |                                                          |                       |
| slope          | Numeric   | The Slope of the Peak Exercise ST Segment               | Range between 0-2                                                         |                  |       |                                                             |                    |
| Ca             | Numeric   | Number of Major Vessels                                 | Range between 0-4                                                         |                |           |                                                         |                        |
| Thal           | Numeric   | Thalassemia                                             | Range between 0-3                                                         |                |          |                                                           |                        |
| Target         | Binary    | If the Target Has Disease or Not                        | 0 = No Disease, 1 = Disease|                                               


Class label:
-target of Heart Attack accurrance 
-not target of Heart Attack accurrance


-------------------
view dataset:
-------------------
```{r}
install.packages("readxl")
library(readxl)
dataset <- read_excel("C:/Users/ragha/OneDrive/سطح المكتب/New folder/Heart Attack Data Set.xlsx")
View(dataset)
```
-----------------------
Summary of dataset:
-----------------------
```{r}
nrow(dataset) 
ncol(dataset)
```

```{r}
summary(dataset$age)
summary(dataset$cp)
summary(dataset$trestbps)
summary(dataset$chol)
summary(dataset$oldpeak)
summary(dataset$thalach)
summary(dataset$slope)
summary(dataset$ca)
summary(dataset$thal)
```
The summary function provides information such as the minimum, 1st quartile, median, mean, 3rd quartile, and maximum values for each variable. It also includes the number of missing values for the thal variable.



---------------------------
Missing values and Null values
---------------------------
```{r}
is.na(dataset)
sum(is.na(dataset))
```

The output is false and 0 this indicates that the element in our dataset is not missing

--------------
Graphs
--------------
Boxplot

check if balanced or not
```{r}
boxplot(dataset$target, 
        ylab= "the target boxplot", 
        main= "Boxplot of target")
```
Since the median is not positioned in the middle of the box plot, it indicates that our dataset is imbalanced, and we need to make it balanced before classification.

```{r}
install.packages("ggplot2")
library(ggplot2)

ggplot(data = dataset, aes(x = "", y = trestbps)) +
  geom_boxplot(fill = "lightgreen", color = "black") +
  labs(
    title = "Box Plot for resting blood pressure  ",
    y = "trestbps",
    x = NULL  # Remove x-axis label (since we only have one category)
  ) +
  theme_minimal()
```
the Box Plot provides valuable insights into the distribution of resting blood pressure between 94- 200, highlighting the high blood pressure experienced by participants and the presence of potential outliers.  In this example, any blood pressure below 94 or above 172 BPM would be considered an outlier.

```{r}
ggplot(data = dataset, aes(x = "", y = thalach)) +
  geom_boxplot(fill = "lightpink", color = "black") +
  labs(
    title = "Box Plot for heart rate achieved ",
    y = "heart rate",
    x = NULL  # Remove x-axis label (since we only have one category)
  ) +
  theme_minimal()
```
the Box Plot provides valuable insights into the distribution of heart rate achieved between 71- 202, highlighting the high rate of heart experienced by participants and the presence of potential outliers.  In this example, any heart rate below 71 or above 202 BPM would be considered an outlier.
Here there only one outlier.


```{r}
ggplot(data = dataset, aes(x = "", y = oldpeak)) +
  geom_boxplot(fill = "purple", color = "black") +
  labs(
    title = "Box Plot for ST depression",
    y = " ST depressione",
    x = NULL  # Remove x-axis label (since we only have one category)
  ) +
  theme_minimal()
```
The St depression boxplot illustrates a lot of outliers above 4.20 that needs to be smoothed to remove the noise

```{r}
ggplot(data = dataset, aes(x = "", y =chol)) +
  geom_boxplot(fill = "lightyellow", color = "black") +
  labs(
    title = "Box Plot for cholestero",
    y = "chol",
    x = NULL  # Remove x-axis label (since we only have one category)
  ) +
  theme_minimal()

```
the Box Plot provides valuable insights into the distribution of serum cholesterol in between 71- 202 mg/dL, highlighting the high serum cholesterol experienced by participants and the presence of potential outliers.  In this example, any serum cholesterol above 390 would be considered an outlier.




we can also detect summary of each attribute from the box plot , and we use box plot to detect  the outlier.

Histogram
```{r}
hist(dataset$chol)
```

The graph represents the frequency of cholesterol in the data set. After observing the values, we concluded that most of the values fall in the range higher than the normal range, as the normal range falls between  125-200.

Scatter plot
```{r}
library(ggplot2)
gg <- ggplot(dataset, aes(x=chol, y=trestbps)) + geom_point(aes(col=target, size=oldpeak)) + labs(subtitle="trestbps Vs chol", y="trestbps", x="chol", title="Scatterplot", 
       caption = "Source: midwest", bins = 30)
plot(gg)
```
The scatter plot represents a comparison between the variables 'trestbps' and 'chol'. The plot displays each data point as a dot on the chart, where the x-axis represents 'trestbps' values and the y-axis represents 'chol' values.

In this scatter plot, we can observe a pattern or trend, indicating that there might be a correlation between 'trestbps' and 'chol'. This could potentially mean that changes in one variable might be associated with changes in the other variable.

The range of 'trestbps' values is from 100 to 175, while the range of 'chol' values is from 230 to 320. This indicates that there are both positive and negative relationships between these two variables.




--------------
 outliers:
--------------

Detecting the outliers:
```{r}
  boxplot.stats (dataset$chol)$out
  boxplot.stats (dataset$trestbps)$out
  boxplot.stats (dataset$oldpeak)$out
  boxplot.stats (dataset$thalach)$out
```


Removing the outliers:
```{r}
outliers <- boxplot (dataset$chol, plot=FALSE) $out
 dataset <- dataset [-which(dataset$chol %in% outliers),]
 boxplot.stats (dataset$chol) $out
 
 outliers <- boxplot (dataset$trestbps, plot=FALSE) $out
 dataset <- dataset [-which(dataset$trestbps %in% outliers),]
 boxplot.stats (dataset$trestbps)$out
 
 outliers <- boxplot (dataset$oldpeak, plot=FALSE) $out
 dataset <- dataset [-which(dataset$oldpeak %in% outliers),]
 boxplot.stats (dataset$oldpeak)$out
 
 outliers <- boxplot (dataset$thalach, plot=FALSE) $out
 dataset <- dataset [-which(dataset$thalach %in% outliers),]
 boxplot.stats (dataset$thalach)$out
```

Description:
First, we identified all outliers in the numeric attributes. Second, we deleted the rows where we find the outliers to produce more accurate dataset that help us to get more accurate results later, finally we checked again to make sure all outliers have been deleted then delete the new outliers that occurred because of the IQR change after deleting the rows in the second step.


| Attribute | Number of Outliers |
|-----------|--------------------|
| chol      | 5                  |
| trestbps  | 9                  |
| oldpeak   | 5                  |
| thalach   | 1                  |


--------------------------------
Transformation and Normalization
--------------------------------
 
Normalization:
```{r}
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

dataset$trestbps <- normalize(dataset$trestbps)
dataset$chol <- normalize(dataset$chol)
dataset$thalach <- normalize(dataset$thalach)
dataset$oldpeak <- normalize(dataset$oldpeak)
dataset$cp <- normalize(dataset$cp)
dataset$oldpeak <- normalize(dataset$oldpeak)
dataset$slope <- normalize(dataset$slope)
dataset$thal <- normalize(dataset$thal)
```

In order to ensure comparability across individuals and eliminate biases resulting from differences in measurement units or scales, we have decided to normalize the following attributes: trestbps, chol, thalach, oldpeak, cp, oldpeak, slope,and thal.

Discretization:

```{r,echo=FALSE}
dataset$age<-cut(dataset$age, breaks = seq(29, 77, by = 10),right=TRUE)
```
In this process, we divide the age values into five intervals with equal width. This allows us to simplify and classify the data, making it more suitable for later use in our model. By discretizing the age values, we effectively group them into five meaningful categories. This transformation will result in a clearer and more organized data set that can be easily understood and utilized in subsequent stages of our model.




• Correlation Analysis:
We will find the correlation between each attribute and the class label (target). For the nominal data we will use chi-square and for the numeric data we will use correlation coefficient. This will help us to determine the most important and correlated attributes to the target.
```{r}
####Sex:

csex=chisq.test(dataset$sex , dataset$target)
print(csex)
####Chest pain type (cp):

ccp=chisq.test(dataset$cp , dataset$target)
print(ccp)
####Fasting blood sugar (fbs):

cfbs=chisq.test(dataset$fbs , dataset$target)
print(cfbs)
####Resting electrocardiographic result (restecg):

crestecg=chisq.test(dataset$restecg , dataset$target)
print(crestecg)
####Exercise induced anginal (exang):

cexang=chisq.test(dataset$exang , dataset$target)
print(cexang)
####The slope of the peak exercise ST segment (slope):

cslope=chisq.test(dataset$slope , dataset$target)
print(cslope)
####Number of major vessels (ca):

cca=chisq.test(dataset$ca , dataset$target)
print(cca)
####A blood disorder (thal):

cthal=chisq.test(dataset$thal , dataset$target)
print(cthal)
####Age: The age attribute after discretization.

cage= chisq.test(dataset$age, dataset$target)
print(cage)
####Resting blood pressure (trestbps)

ctrestbps=chisq.test(dataset$trestbps ,dataset$target)
print(ctrestbps)
```
| Attribute Name                                  | Chi-square Value | Degree of Freedom | Alpha        |
|-------------------------------------------------|------------------|-------------------|--------------|
| A blood disorder (thal)                         | 82.619           | 3                 | 2.2e-16      |
| Chest pain type (cp)                            | 75.192           | 3                 | 3.296e-16    |
| Number of major vessels (ca)                    | 69.764           | 4                 | 2.546e-14    |
| Exercise induced anginal (exang)                | 51.335           | 1                 | 7.786e-13    |
| The slope of the peak exercise ST segment (slope)| 42.008           | 2                 | 7.552e-10    |
| Sex                                             | 27.015           | 1                 | 2.019e-07    |
| Resting blood pressure (trestbps)               | 40.415           | 41                | 0.4965       |
| Resting electrocardiographic result (restecg)   | 9.4074           | 2                 | 0.009062     |
| Age (after discretization)                      | 50.281           | 40                | 0.1278       |
| Fasting blood sugar (fbs)                       | 0.094456         | 1                 | 0.7444       |

    
###Chi-square Results: We will sort the Chi-square values from the highest to the lowest:

1- thal
2- cp
3- ca
4- exang
5- age
6- slope
7- trestbps
8- sex
9- restecg
10- fbs
Description: All of the attributes is dependent to the class label (target) except Fasting blood sugar(fbs) since the p-value is higher than chi-square value (chi-square = 0.094456 <0.7444=p-value)

----------------------
###Correlation coefficient for numeric data:
----------------------
```{r}
####Serum cholestoral (chol):

cchol=cor(dataset$chol ,dataset$target)
print(cchol)
####Maximum heart rate achieved (thalach):

cthalach=cor(dataset$thalach ,dataset$target)
print(cthalach)
####ST depression indicated by exercise relative to rest (oldpeak):

coldpeak=cor(dataset$oldpeak ,dataset$target)
print(coldpeak)
```

Description: The highest correlation coefficient value is ST depression indicated by exercise relative to rest (oldpeak) which is negatively correlated (-0.4357483) with the target. Then, the value of maximum heart rate achieved (thalach) which is positively correlated (0.4243806) with the target. Lastly, the value of Serum cholestoral (chol) which is negatively correlated (-0.1097638) with the target.

From the previous results we decided to delete the Fasting blood sugar (fbs) since it is independent from the class label (target) which means that fasting blood sugar (fbs) doesn't affect our class label (target).


```{r}
dataset <- dataset[, -which(names(dataset) == "fbs")]
head(dataset)
```


```{r}
ncol(dataset)
nrow(dataset)
```
Description: After printing a sample from our new data set we notice that the attribute decrease from 14 to 13 since we remove Fasting blood sugar (fbs) and the rows remain the same.


**print the final preprocessed dataset:**
------------------------------------
```{r}
print(data)
```


we apply chi-square and correlation coefficient to know the attribute that affect our class label in preprocessing ,then started work in classification.
Classification:  is a supervised learning technique where a model is trained on a labeled dataset, meaning the dataset has already been tagged or classified with the correct class labels. 
while we try to increase the accuracy, we try different way one of this way  was add  attribute and  remove attribute and we noticed that the following attribute:slope , doesn’t affect accuracy adding them or removing them doesn’t change anything.


-----------------------
**check if the data set is balanced or not:**
-----------------------
We have previously shown the imbalance of the class label “target ” ( in Pre-processing). Now, we have to balance it to prevent the model from becoming biased towards the majority and thus produce inaccurate results

```{r}
# Identifying the rows of each status
not.target_indices <- which(dataset$target == 0)
target_indices <- which(dataset$target == 1)

# Number of samples for each status class
num_not.target <- length(not.target_indices)
num_target <- length(target_indices)

# Subsampling to balance the data
if (num_not.target > num_target) {
  # Subsample the "not target" status to match the "target" count
  sampled_not.target_indices <- sample(not.target_indices, num_target)
  balanced_dataset <- rbind(dataset[sampled_not.target_indices, ], dataset[target_indices, ])
} else {
  # Subsample the "target" status to match the "not target" count
  sampled_target_indices <- sample(target_indices, num_not.target)
  balanced_dataset <- rbind(dataset[not.target_indices, ], dataset[sampled_target_indices,])
}
```

Verify Class Label Balance



```{r}
# Count of each class in the original dataset
original_class_counts <- table(dataset$target)

# Count of each class in the balanced dataset
balanced_class_counts <- table(balanced_dataset$target)

# Display the counts
print("Original Dataset Class Counts:")
print(original_class_counts)
```


```{r}
print("Balanced Dataset Class Counts:")
print(balanced_class_counts)
```
There are 159 rows for target  (majority), and 125 rows for not target  (minority) before balancing. After balancing, there are 125 rows for target  and 125 rows for not target  showing a balnced class label ready for classification.


-------------------
**Classification:**
-------------------

Load the libraries:
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(partykit)
library(RWeka)
library(tidymodels)
library(tidyr)
library(MASS)
library(rpart)
library(rpart.plot)
library(caret)
```
## split the data in training and test data (75/25)

```{r}
set.seed(123)
data_split <- initial_split(dataset, prop = 0.75)
train_data <- training(data_split)
test_data <- testing(data_split)
```


## build decision tree for each partition and measure

```{r}
tree_1_ig <- rpart(as.factor(target) ~ ., data = train_data, method = "class", parms = list(split = "information"))

tree_1_gini <- rpart(target ~ ., data = train_data, method = "class", parms = list(split = "gini"))

tree_1_gr <- rpart(target ~ ., data = train_data, method = "class", parms = list(split = "anova"))
```







##plot the decision trees for each measure(IG)

```{r}
rpart.plot(tree_1_ig, main = "Decision Tree for 75/25 Partition using Information Gain")
```
This s a decision tree diagram for a 75/25 partition using information gain.The result shows that Chest pain type (cp) had the highest information gain value so it was represented in first level (root).The tree contains 7 variables with 8 splits. Splits have occurred on the variables Cp, Ca ,thalach, thal,sex and chol and oldpeak.


```{r}
#predict the target on the test set
pred_1_ig <- predict(tree_1_ig, newdata = test_data, type = "class")

pred <- as.factor(pred_1_ig) 
actual <- as.factor(test_data$target)

confusionMatrix(pred, actual)

#create a confusion matrix
table(pred_1_ig, test_data$target)

#calculate the overall accuracy
mean(pred_1_ig ==  test_data$target)
```



```{r}
# Manually calculate precision
precision <- 22 / (22 + 5)

# Print precision
print(paste("Precision:", precision))
```
| Evaluation Method | Value  |
|-------------------|--------|
| Accuracy          | 80%    |
| Precision         | 81.4%  |
| Sensitivity       | 70.9%  |
| Specificity       | 87.5%  |


## plot the decision trees for each measure(Gini index)

```{r}
rpart.plot(tree_1_gini, main = "Decision Tree for 75/25 Partition using gini index")
```
This s a decision tree diagram for a 75/25 partition using  Gini index .The result shows that Chest pain type (cp) had the highest ΔGini value so it was represented in first level (root).The tree contains 5 variables with 6 splits. Splits have occurred on the variables Cp, Ca ,thalach, thal, and oldpeak.

```{r}
#predict the target on the test set
tree_1_gini <- predict(tree_1_gini, newdata = test_data, type = "class")

#create a confusion matrix
table(tree_1_gini, test_data$target)

pred <- as.factor(tree_1_gini) 
actual <- as.factor(test_data$target)

confusionMatrix(pred, actual)

#calculate the overall accuracy
mean(tree_1_gini ==  test_data$target)
```


```{r}
# Manually calculate precision
precision <- 24 / (24 + 4)

# Print precision
print(paste("Precision:", precision))
```
| Evaluation Method | Value  |
|-------------------|--------|
| Accuracy          | 84.5%  |
| Precision         | 85.7%  |
| Sensitivity       | 77.4%  |
| Specificity       | 90%    |


## plot the decision trees for each measure(GINI ratio)

```{r}
rpart.plot(tree_1_gr, main = "Decision Tree for 75/25 Partition using gini ratio")
```
This s a decision tree diagram for a 75/25 partition using  Gini ratio .The result shows that Chest pain type (cp) had the highest Gini ratio value so it was represented in first level (root).The tree contains 5 variables with 6 splits. Splits have occurred on the variables Cp, Ca ,thalach, thal,and oldpeak.


```{r}
#predict the target on the test set
tree_1_gr <- predict(tree_1_gr, newdata = test_data, type = "class")

#create a confusion matrix
table(tree_1_gr, test_data$target)

pred <- as.factor(tree_1_gr) 
actual <- as.factor(test_data$target)

confusionMatrix(pred, actual)



#calculate the overall accuracy
mean(tree_1_gr ==  test_data$target)
```

```{r}
# Manually calculate precision
precision <- 24 / (24 + 4)

# Print precision
print(paste("Precision:", precision))
```
| Evaluation Method | Value  |
|-------------------|--------|
| Accuracy          | 84.5%  |
| Precision         | 85.7%  |
| Sensitivity       | 77.4%  |
| Specificity       | 90%    |





## C4.5 algorithm (75/25)

```{r}
library(RWeka)
# load the package
library(RWeka)
# load data
# fit model
tree <- J48(as.factor(target) ~ ., data = train_data)
# summarize the fit
summary(tree)
# make predictions
predictions <- predict(tree, test_data)
# summarize accuracy
table(predictions, test_data$target)
#calculate the overall accuracy
mean(predictions ==  test_data$target)
library(partykit)
```


## Cart algorithm (75/25)

```{r}
# load the package
library(ipred)
# fit model
tree <- Bagging (as.factor(target) ~ ., data = train_data)
summary(tree)
# make predictions
predictions <- predict(tree, test_data)
# summarize accuracy
table(predictions, test_data$target)
#calculate the overall accuracy
mean(predictions ==  test_data$target)
```




## split the data in training and test data (80/20)


```{r}
set.seed(123)
data_split <- initial_split(dataset, prop = 0.80)
train_data <- training(data_split)
test_data <- testing(data_split)
```

## build decision tree for each partition and measure

```{r}
tree_1_ig <- rpart(target ~ ., data = train_data, method = "class", parms = list(split = "information"))

tree_1_gini <- rpart(target ~ ., data = train_data, method = "class", parms = list(split = "gini"))

tree_1_gr <- rpart(target ~ ., data = train_data, method = "class", parms = list(split = "anova"))
```

--------


## plot the decision trees for each measure(IG)

```{r}
rpart.plot(tree_1_ig, main = "Decision Tree for 80/20 Partition using Information Gain")
```
This s a decision tree diagram for a 80/20 partition using information gain.The result shows that Chest pain type (cp) had the highest information gain value so it was represented in first level (root).The tree contains 4 variables with 6 splits. Splits have occurred on the variables Cp, Ca ,thal, and oldpeak.


```{r}
#predict the species on the test set
pred_1_ig <- predict(tree_1_ig, newdata = test_data, type = "class")

#create a confusion matrix
table(pred_1_ig, test_data$target)

pred <- as.factor(pred_1_ig) 
actual <- as.factor(test_data$target)

confusionMatrix(pred, actual)

#calculate the overall accuracy
mean(pred_1_ig ==  test_data$target)
```



```{r}
# Manually calculate precision
precision <- 17 / (17 + 4)

# Print precision
print(paste("Precision:", precision))
```
| Evaluation Method | Value  |
|-------------------|--------|
| Accuracy          | 77.1%  |
| Precision         | 80.9%  |
| Sensitivity       | 65.3%  |
| Specificity       | 87%    |


## plot the decision trees for each measure(GINI index)

```{r}
rpart.plot(tree_1_gini, main = "Decision Tree for 80/20 Partition using gini index")
```
his s a decision tree diagram for a 80/20partition using  Gini index .The result shows that Chest pain type (cp) had the highest ΔGini value so it was represented in first level (root).The tree contains 5 variables with 8 splits. Splits have occurred on the variables Cp, Ca ,thalach, thal,and oldpeak.


```{r}
#predict the target on the test set
tree_1_gini <- predict(tree_1_gini, newdata = test_data, type = "class")

#create a confusion matrix
table(tree_1_gini, test_data$target)

pred <- as.factor(tree_1_gini) 
actual <- as.factor(test_data$target)

confusionMatrix(pred, actual)

#calculate the overall accuracy
mean(tree_1_gini ==  test_data$target)
```

```{r}
# Manually calculate precision
precision <- 16 / (16 + 4)

# Print precision
print(paste("Precision:", precision))
```
| Evaluation Method | Value  |
|-------------------|--------|
| Accuracy          | 75.4%  |
| Precision         | 80%    |
| Sensitivity       | 61.5%  |
| Specificity       | 87%    |


##plot the decision trees for each measure(GINI ratio)

```{r}
rpart.plot(tree_1_gr, main = "Decision Tree for 80/20 Partition using gini ratio")
```

This s a decision tree diagram for a 80/20 partition using  Gini ratio .The result shows that Chest pain type (cp) had the highest Gini ratio value so it was represented in first level (root).The tree contains 5 variables with 8 splits. Splits have occurred on the variables Cp, Ca ,thalach, thal,and oldpeak.

```{r}
#predict the target on the test set
tree_1_gr <- predict(tree_1_gr, newdata = test_data, type = "class")

#create a confusion matrix
table(tree_1_gr, test_data$target)

pred <- as.factor(tree_1_gr) 
actual <- as.factor(test_data$target)

confusionMatrix(pred, actual)

#calculate the overall accuracy
mean(tree_1_gr ==  test_data$target)
```


```{r}
# Manually calculate precision
precision <- 16 / (16 + 4)

# Print precision
print(paste("Precision:", precision))
```
| Evaluation Method | Value  |
|-------------------|--------|
| Accuracy          | 75.4%  |
| Precision         | 80%    |
| Sensitivity       | 61.5%  |
| Specificity       | 87%    |


the confusion matrix of the IG model showed the accuracy of 77%, th gini ratio showed 75.4%, while the gini index showed 75.4% accuracy looking at the accuracy level we can see that IG model performed better.


## C4.5 algorithm (80/20)

```{r}
library(RWeka)
# load the package
library(RWeka)
# load data
# fit model
tree <- J48(as.factor(target) ~ ., data = train_data)
# summarize the fit
summary(tree)
# make predictions
predictions <- predict(tree, test_data)
# summarize accuracy
table(predictions, test_data$target)
#calculate the overall accuracy
mean(predictions ==  test_data$target)
library(partykit)
```





## Cart algorithm (80/20)

```{r}
# load the package
library(ipred)
# fit model
tree <- Bagging (as.factor(target) ~ ., data = train_data)
summary(tree)
# make predictions
predictions <- predict(tree, test_data)
# summarize accuracy
table(predictions, test_data$target)
#calculate the overall accuracy
mean(predictions ==  test_data$target)
```




## split the data in training and test data (60/40)

```{r}
set.seed(123)
data_split <- initial_split(dataset, prop = 0.60)
train_data <- training(data_split)
test_data <- testing(data_split)
```


## build decision tree for each partition and measure
```{r}
tree_1_ig <- rpart(target ~ ., data = train_data, method = "class", parms = list(split = "information"))

tree_1_gini <- rpart(target ~ ., data = train_data, method = "class", parms = list(split = "gini"))

tree_1_gr <- rpart(target ~ ., data = train_data, method = "class", parms = list(split = "anova"))
```



##plot the decision trees for each measure(IG)

```{r}
rpart.plot(tree_1_ig, main = "Decision Tree for 60/40 Partition using Information Gain")
```
This is a decision tree diagram for a 60/40 partition using information gain.The result shows that Chest pain type (cp) had the highest information gain value so it was represented in first level (root).The tree contains 4 variables with 6 splits. Splits have occurred on the variables Cp, Ca ,thalach and oldpeak.

```{r}
#predict the target on the test set
pred_1_ig <- predict(tree_1_ig, newdata = test_data, type = "class")

#create a confusion matrix
table(pred_1_ig, test_data$target)

pred <- as.factor(pred_1_ig) 
actual <- as.factor(test_data$target)

confusionMatrix(pred, actual)

#calculate the overall accuracy
mean(pred_1_ig ==  test_data$target)
```


```{r}
# Manually calculate precision
precision <- 37/ (37+ 13)

# Print precision
print(paste("Precision:", precision))
```
| Evaluation Method | Value  |
|-------------------|--------|
| Accuracy          | 73.6%  |
| Precision         | 74%    |
| Sensitivity       | 68.5%  |
| Specificity       | 78.3%  |


## plot the decision trees for each measure(GINI index)

```{r}
rpart.plot(tree_1_gini, main = "Decision Tree for 60/40 Partition using gini index")
```
This is a decision tree diagram for a 60/40 partition using  Gini index .The result shows that Chest pain type (cp) had the highest ΔGini value so it was represented in first level (root).The tree contains 5 variables with 6 splits. Splits have occurred on the variables Cp, Ca ,thalach, thal, and oldpeak.


```{r}
#predict the target on the test set
tree_1_gini <- predict(tree_1_gini, newdata = test_data, type = "class")

#create a confusion matrix
table(tree_1_gini, test_data$target)

pred <- as.factor(tree_1_gini) 
actual <- as.factor(test_data$target)

confusionMatrix(pred, actual)

#calculate the overall accuracy
mean(tree_1_gini ==  test_data$target)
```



```{r}
# Manually calculate precision
precision <- 37/ (37+ 13)

# Print precision
print(paste("Precision:", precision))
```
| Evaluation Method | Value  |
|-------------------|--------|
| Accuracy          | 73.6%  |
| Precision         | 74%    |
| Sensitivity       | 68.5%  |
| Specificity       | 78.3%  |



## plot the decision trees for each measure(GINI ratio)

```{r}
rpart.plot(tree_1_gr, main = "Decision Tree for 60/40 Partition using gini ratio")
```
This is a decision tree diagram for a 60/40 partition using  Gini ratio .The result shows that Chest pain type (cp) had the highest Gini ratio value so it was represented in first level (root).The tree contains 4 variables with 6 splits. Splits have occurred on the variables Cp, Ca ,thalach and oldpeak.


## predict the model on the test set

```{r}
#predict the target on the test set
tree_1_gr <- predict(tree_1_ig, newdata = test_data, type = "class")

#create a confusion matrix
table(tree_1_gr, test_data$target)
pred <- as.factor(tree_1_gr) 
actual <- as.factor(test_data$target)
confusionMatrix(pred, actual)
#calculate the overall accuracy
mean(tree_1_gr ==  test_data$target)
```
the confusion matrix of the IG model showed the accuracy of 77%, th gini ration showed 76%, while the gini index showed 77% accuracy looking at the accuracy level we can that the IG model and gini index model measure performed better at classification.

```{r}
# Manually calculate precision
precision <- 37/ (37+13)

# Print precision
print(paste("Precision:", precision))
```
| Evaluation Method | Value  |
|-------------------|--------|
| Accuracy          | 73.6%  |
| Precision         | 74%    |
| Sensitivity       | 68.5%  |
| Specificity       | 78.3%  |

## C4.5 algorithm (60/40)

```{r}
library(RWeka)
# load the package
library(RWeka)
# load data
# fit model
tree <- J48(as.factor(target) ~ ., data = train_data)
# summarize the fit
summary(tree)
# make predictions
predictions <- predict(tree, test_data)
# summarize accuracy
table(predictions, test_data$target)
#calculate the overall accuracy
mean(predictions ==  test_data$target)
library(partykit)
plot(tree)
```


## Cart algorithm (60/40)

```{r}
# load the package
library(ipred)
# fit model
tree <- Bagging (as.factor(target) ~ ., data = train_data)
summary(tree)
# make predictions
predictions <- predict(tree, test_data)
# summarize accuracy
table(predictions, test_data$target)
#calculate the overall accuracy
mean(predictions ==  test_data$target)
```

---------------------
**Evaluation and Comparison in classification:**
---------------------

We applied several methods to evaluate classification models which are: accuracy, sensitivity  , specificity, and precision. Accuracy means the ability of the model to classify the data correctly. It’s used to evaluate the performance of the model based on the test data. Sensitivity means the ability of the model to correctly identify positive tuples. Specificity is the opposite of the sensitivity. It means the ability of the model to correctly identify negative tuples. Finally, precision means the proportion of correctly predicted positive tuples out of all tuples predictive as positive.

Information Gain:
It measures the amount of information provided by a feature, information gain is used to determine the best feature which has the highest value to split the data at each node of the tree.

Gini index:
It is a measurement used in classifying the data and building decision trees, it is used to determine the splitting attribute. The gini index measure the impurity of the data and the attribute with the lowest gini index value will have the highest reduction impurity and will be chosen as the split attribute.

Gain ratio:
It is a measure used in decision tree algorithms to select the best feature that has the highest value for splitting data. It takes into consideration both the information gain and the split information of a feature.




 **75% Training, 25% Testing :**
 
|              | IG | IG Ratio | Gini Index | 
|--------------|----|----------|------------|
| **Accuracy** | 80%| 84.5%    | 84.5%      | 
| **Precision**|81.4%| 85.7%   | 85.7%      | 
| **Sensitivity**|70.9%| 77.4%  | 77.4%      | 
| **Specificity**|87.5%| 90%    | 90%        | 




 **80% Training, 20% Testing :**

|              | IG | IG Ratio | Gini Index | 
|--------------|----|----------|------------|
| **Accuracy** | 77%| 75.4%    | 75.4%      | 
| **Precision**|80.9%| 80%     | 80%        | 
| **Sensitivity**|65%| 61.5%   | 61.5%      |
| **Specificity**|87%| 87%     | 87%        | 



**60% Training, 40% Testing :**

|              | IG | IG Ratio | Gini Index | 
|--------------|----|----------|------------|
| **Accuracy** | 73.6%| 73.6%  | 73.6%      |
| **Precision**|74%  | 74%    | 74%        |
| **Sensitivity**|68.5%| 68.5%  | 68.5%      |
| **Specificity**|78.3%| 78.3%  | 78.3%      |


 
**Summary for 75% training set and 25% testing set:**
------------------------------------------------
The best algorithm based on accuracy, precision, sensitivity, and specificity is Gini index and Information Gain Ratio (IG ratio) with an accuracy of 84.5%


**Summary for 80% training set and 20% testing set:**
------------------------------------------------
The best algorithm based on accuracy, precision, sensitivity, and specificity is Information Gain (IG) with an accuracy of 77%



**Summary for 60% training set and 40% testing set:**
------------------------------------------------
All algorithms perform equally with an accuracy of 73.6%.

------------------------------------------------
**Conclusion:**
-----------------------------------------------
we noticed that gini ratio and gini index algorithms ine ach partition have the same accuracy .And the highest partition accuracy is 75/25 since has the highest accuracy in both algorithm gini ratio and gini index with value=84.5%.

we noticed in the last partition 60/40 all algorithms have the same accuracy.


The best-performing algorithm overall is both Information Gain Ratio (IG ratio) and Gini index because they have an average of accuracy  79.33%. they consistently performs well across different partitions based on the provided metrics.



--------------------------
**Finding in classification:**
--------------------------
In classification, We perform multiple decision trees using different partitioning splits and different attribute selection method we chose that had the best accuracy which is partitioning data into 75% training and 25% testing with Gini index and Gain ratio  attribute selection. It had the highest accuracy (84.5 %) indicating that the model had the best performance in predicting the correct class labels(targeted or not targeted to heart attack ). In the other hand, the error rate was low (15.4%) since it equals (1-accuracy) and it shows the percent of incorrect predictions. For sensitivity it was (77.4%) which is high and tells that the model can well predict the positive instances (targeted to heart attack) and for specificity it was (90%) which is higher than the sensitivity and it tells how the model can predict the positive instances (targeted to heart attack). Lastly, the precision was (85.7 %) indicating the true positive instances over all that instances the model predicted as positive.

The decision tree had 7 pathes from the root to the leaves so, we will have 7 rules. Rules: 
1- if number of major vessels (cp) <1 and (ca) >= 1 then not target to have heart attack (target= 0) .
2- if number of major vessels (cp) < 1 and(ca) < 1 and (thal) > = 3 and ( oldpeak ) >= 0.65  then  not target to have heart attack(target= 0) .

3- if number of major vessels (cp) < 1  and (ca) < 1 and (thal )< 3 and ( oldpeak ) < 0.65  then target to have heart attack(target= 1 ). 
 
4- if number of major vessels (cp) < 1  and (ca) < 1 and (thal )>= 3  then target to have heart attack(target= 1 ). 

5- if number of major vessels (cp ) > =1  and (thal )>= 3  and (thalach) <143 then not target to have heart attack (target= 0) . 

6- if number of major vessels (cp ) > =1  and (thal )>= 3  and (thalach) >= 143 then target to have heart attack (target= 1) . 

7- if number of major vessels (cp ) > =1  and (thal )< 3  then target to have heart attack (target= 1) . 

The decision tree illustrates that 5 attributes were used in predicting the class label which are ( cp ,cab,thal ,thalach and oldpeak ) where the other attributes had no effect in the class label prediction.



---------------------
 Evaluation and Comparison in clustering:
---------------------


|                                     | K = 2     | K = 3     | K = 4     |
|-------------------------------------|-----------|-----------|-----------|
| Average Silhouette width            | 0.16      | 0.11      | 0.12      |
| total within-cluster sum of squares | 3264.308  | 3233.463  | 2808.383  |
| BCubed precision                    | 0.70      | 0.63      | 0.67      |
| BCubed recall                       | 0.72      | 0.51      | 0.36      |


--------------------------
Finding in clustring:
--------------------------

For K=2 : The average silhouette width was 0.16, which shows us that the objects in a cluster were grouped together closely and less overlapping. Precision was 0.70 and recall was 0.72.This shows us a good capability to accurately capture instances within clusters and recognize relevant points. However, The total within-cluster sum of squares is 3417.422, which tells us that the points are far from  the centroid. large distance tells us that the points are farther away from the centroid, which is not wanted for a good cluster.

For K=3: The average silhouette width was 0.11, which shows us that the objects in a cluster were not really grouped together closely and there is likely more overlapping. Precision was 0.63 and recall was 0.51.This shows us a lesser  capability to accurately capture instances within clusters and recognize relevant points.The total within-cluster sum of squares is 3201.26, which tells us that the points are also farther from  the centroid. large distance tells us that the points are farther away from the centroid, which is not wanted for a good cluster.

For K=4: The average silhouette width was 0.12, which shows us that the objects in a cluster were not really grouped together closely and there is likely more overlapping. Precision was 0.67 and recall was 0.36.This shows us a lesser  capability to accurately capture instances within clusters and recognize relevant points. However, The total within-cluster sum of squares is 2996.77, which tells us that the points are closer to  the centroid. The closer the points in a cluster are to the centroid, the better. A small distance tells us that the points are closer to the centroid, which is wanted for a good cluster.


In conclusuin, Bcubed and silhouette methods show us that  2 means is the best choice for clustering, and total within-cluster sum of square was the only method that told us that 4 means is the best.But in our case,this is not accurate because low WSS is supposed to mean that the clusters don’t really overlap but in our case 4 means shows us that there is a lot of overlapping (although it has the least wss value) in the clusters and 2 means gives us the least overlapping to almost no overlapping (although it has the most wss value). so we must discard what  the total within-cluster sum of square is telling us and choose the 2 means as the optimal number of clusters.




------------------------------------------------------------------------------

Both approaches (Classification and Clustering)were helpful for building models for our data set that can help in reaching our goal which is detecting people from heart attacks, but since our data set includes class label which is target, classification was considered the best option to predict the possibilities of having heart attack based on the attributes also the model had great accuracy.



------------
Refrences:
------------
[1]“rstudio - ggplot2 library installation in R Studio,” Stack Overflow. https://stackoverflow.com/questions/36582347/ggplot2-library-installation-in-r-studio.
[2]“boxplot function - RDocumentation,” www.rdocumentation.org. https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/boxplot.
[3]“R Documentation and manuals | R Documentation,” Rdocumentation.org, 2019. https://www.rdocumentation.org/.

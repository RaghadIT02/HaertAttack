---
output:
  html_document: default
  pdf_document: default
---
---
title: "R Notebook"
output:
  pdf_document: default
  html_notebook: default
-----------------------
|Name           | ID        |
----------------------------|
|Taif alrubeaan | 442202301 |
|Raghad Alboqami| 442200455 |
|Leena Alzahrani|           |
|Sarah Alghanim |           |


1 Problem
Recently, Cardiovascular diseases rate has been increased, which leads death globally. Early detection of the symptoms is important to prevent the disease from fully developing and reduce premature mortality. In our project, we will study and analyze patients’ data that will help us well in identifying symptoms that you don't recognize as a sign of a heart disease and help many people to take precautions by predicting the possibility of having a Heart attack.

2 Data Mining Task
 In our project we will use two data mining tasks to help us predict the possibility of having cardiovascular diseases which are classification and clustering. For classification we will train our model to be able to classify if the patient has cardiovascular diseases or not using (target) class based on a set of medical examinations like blood pressure, serum cholesterol , ST depression, etc. For the clustering our model will create a set of clusters for the patient who have similar characteristics, then these clusters will be used to predict new patients’ results.
 


3 Data 
The source :
 https://www.kaggle.com/datasets/andrewmvd/ocular-disease-recognition-odir5k
Number of objects: 303
 Number of attributes: 14



• Attribute characteristics:

| Attribute name | Data type | Description                                             | possible values                                                           |
|------------------|------------------|-------------------|------------------|
| age            | Numeric   | Age (in years)                                          | range between 29-77                                                       |
| sex            | Binary    | gender                                                  | 1 = male; 0 =female                                                       |
| cp             | Numeric   | Chest Pain type                                         | 1: typical angina,2: atypical angina,3: non-anginal pain ,4: asymptomatic |
| trestbps       | Numeric   | resting blood pressure                                  | range beteen 94-200                                                       |
| chol           | Numeric   | serum cholesterol in mg/dL                              | range between 126-564                                                     |
| fbs            | Binary    | fasting blood sugar \> 120 mg/dL (likely to be diabetic | 1=true,0=false                                                            |
| Restecg        | Binary    | Resting electrocardiogram                               | 0=normal,1=abnormal                                                       |
| Oldpeak        | Numeric   | ST depression                                           | range between 0-6.20                                                      |
| thalach        | Numeric   | heart rate achieved                                     | range between 71-202                                                      |
| exang          | Binary    | exercise induced angina                                 | 1 = yes; 0 = no                                                           |
| slope          | Numeric   | the slope of the peak exercise ST segment               | range between 0-2                                                         |
| Ca             | Numeric   | number of major vessels                                 | range between 0-4                                                         |
| Thal           | Numeric   | Thalassemia                                             | range between 0-3                                                         |
| Target         | Binary    | if the target has disease or not                        | 0 = no disease, 1 = disease                                               |

Class label:
-target of Heart Attack accurrance 
-not target of Heart Attack accurrance


-------------------
view dataset:
-------------------
```{r}
install.packages("readxl")
library(readxl)
dataset <- read_excel("C:/Users/ragha/OneDrive/سطح المكتب/New folder/Heart Attack Data Set.xlsx")
View(dataset)
```



-----------------------
Summary of dataset:
-----------------------
```{r}
nrow(dataset) 
ncol(dataset)
```

```{r}
summary(dataset$age)
summary(dataset$cp)
summary(dataset$trestbps)
summary(dataset$chol)
summary(dataset$oldpeak)
summary(dataset$thalach)
summary(dataset$slope)
summary(dataset$ca)
summary(dataset$thal)
```
The summary function provides information such as the minimum, 1st quartile, median, mean, 3rd quartile, and maximum values for each variable. It also includes the number of missing values for the thal variable.



---------------------------
Missing values and Null values
---------------------------
```{r}
is.na(dataset)
sum(is.na(dataset))
```

The output is false and 0 this indicates that the element in our dataset is not missing

--------------
Graphs
--------------
Boxplot

check if balanced or not
```{r}
boxplot(dataset$target, 
        ylab= "the target boxplot", 
        main= "Boxplot of target")
```




```{r,echo=FALSE}
install.packages("ggplot2")
library(ggplot2)

ggplot(data = dataset, aes(x = "", y = trestbps)) +
  geom_boxplot(fill = "lightgreen", color = "black") +
  labs(
    title = "Box Plot for resting blood pressure  ",
    y = "trestbps",
    x = NULL  # Remove x-axis label (since we only have one category)
  ) +
  theme_minimal()
```
the Box Plot provides valuable insights into the distribution of resting blood pressure between 94- 200, highlighting the high blood pressure experienced by participants and the presence of potential outliers.  In this example, any blood pressure below 94 or above 172 BPM would be considered an outlier.

```{r,echo=FALSE}
ggplot(data = dataset, aes(x = "", y = thalach)) +
  geom_boxplot(fill = "lightpink", color = "black") +
  labs(
    title = "Box Plot for heart rate achieved ",
    y = "heart rate",
    x = NULL  # Remove x-axis label (since we only have one category)
  ) +
  theme_minimal()
```
the Box Plot provides valuable insights into the distribution of heart rate achieved between 71- 202, highlighting the high rate of heart experienced by participants and the presence of potential outliers.  In this example, any heart rate below 71 or above 202 BPM would be considered an outlier.
Here there only one outlier.


```{r,echo=FALSE}
ggplot(data = dataset, aes(x = "", y = oldpeak)) +
  geom_boxplot(fill = "purple", color = "black") +
  labs(
    title = "Box Plot for ST depression",
    y = " ST depressione",
    x = NULL  # Remove x-axis label (since we only have one category)
  ) +
  theme_minimal()
```
The St depression boxplot illustrates a lot of outliers above 4.20 that needs to be smoothed to remove the noise

```{r,echo=FALSE}
ggplot(data = dataset, aes(x = "", y =chol)) +
  geom_boxplot(fill = "lightyellow", color = "black") +
  labs(
    title = "Box Plot for cholestero",
    y = "chol",
    x = NULL  # Remove x-axis label (since we only have one category)
  ) +
  theme_minimal()

```
the Box Plot provides valuable insights into the distribution of serum cholesterol in between 71- 202 mg/dL, highlighting the high serum cholesterol experienced by participants and the presence of potential outliers.  In this example, any serum cholesterol above 390 would be considered an outlier.




we can also detect summary of each attribute from the box plot , and we use box plot to detect  the outlier.

Histogram
```{r}
hist(dataset$chol)
```

The graph represents the frequency of cholesterol in the data set. After observing the values, we concluded that most of the values fall in the range higher than the normal range, as the normal range falls between  125-200.

Scatter plot
```{r}
library(ggplot2)
gg <- ggplot(dataset, aes(x=chol, y=trestbps)) + geom_point(aes(col=target, size=oldpeak)) + labs(subtitle="trestbps Vs chol", y="trestbps", x="chol", title="Scatterplot", 
       caption = "Source: midwest", bins = 30)
plot(gg)
```
The scatter plot represents a comparison between the variables 'trestbps' and 'chol'. The plot displays each data point as a dot on the chart, where the x-axis represents 'trestbps' values and the y-axis represents 'chol' values.

In this scatter plot, we can observe a pattern or trend, indicating that there might be a correlation between 'trestbps' and 'chol'. This could potentially mean that changes in one variable might be associated with changes in the other variable.

The range of 'trestbps' values is from 100 to 175, while the range of 'chol' values is from 230 to 320. This indicates that there are both positive and negative relationships between these two variables.


------------
Bar plot
------------
 There is no point in using bar plots because the frequency of one symptom (ex: high blood pressure alone ) is not enough to decide whether the patients are likely to have a heart attack or not

--------------
 outliers:
--------------

Detecting the outliers:
```{r}
  boxplot.stats (dataset$chol)$out
  boxplot.stats (dataset$trestbps)$out
  boxplot.stats (dataset$oldpeak)$out
  boxplot.stats (dataset$thalach)$out
```


Removing the outliers:
```{r}
outliers <- boxplot (dataset$chol, plot=FALSE) $out
 dataset <- dataset [-which(dataset$chol %in% outliers),]
 boxplot.stats (dataset$chol) $out
 
 outliers <- boxplot (dataset$trestbps, plot=FALSE) $out
 dataset <- dataset [-which(dataset$trestbps %in% outliers),]
 boxplot.stats (dataset$trestbps)$out
 
 outliers <- boxplot (dataset$oldpeak, plot=FALSE) $out
 dataset <- dataset [-which(dataset$oldpeak %in% outliers),]
 boxplot.stats (dataset$oldpeak)$out
 
 outliers <- boxplot (dataset$thalach, plot=FALSE) $out
 dataset <- dataset [-which(dataset$thalach %in% outliers),]
 boxplot.stats (dataset$thalach)$out
 
```

Description:
First, we identified all outliers in the numeric attributes. Second, we deleted the rows where we find the outliers to produce more accurate dataset that help us to get more accurate results later, finally we checked again to make sure all outliers have been deleted then delete the new outliers that occurred because of the IQR change after deleting the rows in the second step.


Attribute  |Number of outliers
------------------------------
chol       |5                 |
trestbps   |9                 |
oldpeak    |5                 |
thalach    |1                 |


--------------------------------
Transformation and Normalization
--------------------------------
 
Normalization:
```{r}
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

dataset$trestbps <- normalize(dataset$trestbps)
dataset$chol <- normalize(dataset$chol)
dataset$thalach <- normalize(dataset$thalach)
dataset$oldpeak <- normalize(dataset$oldpeak)
dataset$cp <- normalize(dataset$cp)
dataset$oldpeak <- normalize(dataset$oldpeak)
dataset$slope <- normalize(dataset$slope)
dataset$thal <- normalize(dataset$thal)
```

In order to ensure comparability across individuals and eliminate biases resulting from differences in measurement units or scales, we have decided to normalize the following attributes: trestbps, chol, thalach, oldpeak, cp, oldpeak, slope,and thal.


------------------------------------
feature selection
------------------------------------
we did not applly feature selection beacause the number of attribute is small and we will  neeed them
------------------------------------
• Correlation Analysis:
We will find the correlation between each attribute and the class label (target). For the nominal data we will use chi-square and for the numeric data we will use correlation coefficient. This will help us to determine the most important and correlated attributes to the target.
```{r}
####Sex:

csex=chisq.test(dataset$sex , dataset$target)
print(csex)
####Chest pain type (cp):

ccp=chisq.test(dataset$cp , dataset$target)
print(ccp)
####Fasting blood sugar (fbs):

cfbs=chisq.test(dataset$fbs , dataset$target)
print(cfbs)
####Resting electrocardiographic result (restecg):

crestecg=chisq.test(dataset$restecg , dataset$target)
print(crestecg)
####Exercise induced anginal (exang):

cexang=chisq.test(dataset$exang , dataset$target)
print(cexang)
####The slope of the peak exercise ST segment (slope):

cslope=chisq.test(dataset$slope , dataset$target)
print(cslope)
####Number of major vessels (ca):

cca=chisq.test(dataset$ca , dataset$target)
print(cca)
####A blood disorder (thal):

cthal=chisq.test(dataset$thal , dataset$target)
print(cthal)
####Age: The age attribute after discretization.

cage= chisq.test(dataset$age, dataset$target)
print(cage)
####Resting blood pressure (trestbps)

ctrestbps=chisq.test(dataset$trestbps ,dataset$target)
print(ctrestbps)

```
|  Attribute name | Chi-square value|Degree of freedom|Alpa|             
  |-----------------|-----------------|-----------------|-----------------|
  | A blood disorder (thal) | 82.619|3|2.2e-16|
   | Chest pain type (cp)    | 75.192 |3| 3.296e-16| 
  | Number of major vessels (ca)| 69.764|4|2.546e-14|
  | Exercise induced anginal (exang)| 51.335 |1|7.786e-13|
  |The slope of the peak exercise ST segment (slope)| 42.008|2|7.552e-10|
  | Sex            | 27.015       | 1| 2.019e-07|
  |Resting blood pressure (trestbps)|  40.415|41|0.4965|
  | Resting electrocardiographic result (restecg)| 9.4074 |2|0.009062|     
  |Age(after discretization)| 50.281  |40|0.1278|      
  | Fasting blood sugar (fbs)    | 0.094456|1|0.7444|
    
###Chi-square Results: We will sort the Chi-square values from the highest to the lowest:

1- thal
2- cp
3- ca
4- exang
5- age
6- slope
7- trestbps
8- sex
9- restecg
10- fbs
Description: All of the attributes is dependent to the class label (target) except Fasting blood sugar(fbs) since the p-value is higher than chi-square value (chi-square = 0.094456 <0.7444=p-value)

----------------------
###Correlation coefficient for numeric data:
----------------------
```{r}
####Serum cholestoral (chol):

cchol=cor(dataset$chol ,dataset$target)
print(cchol)
####Maximum heart rate achieved (thalach):

cthalach=cor(dataset$thalach ,dataset$target)
print(cthalach)
####ST depression indicated by exercise relative to rest (oldpeak):

coldpeak=cor(dataset$oldpeak ,dataset$target)
print(coldpeak)
```

Description: The highest correlation coefficient value is ST depression indicated by exercise relative to rest (oldpeak) which is negatively correlated (-0.4357483) with the target. Then, the value of maximum heart rate achieved (thalach) which is positively correlated (0.4243806) with the target. Lastly, the value of Serum cholestoral (chol) which is negatively correlated (-0.1097638) with the target.

From the previous results we decided to delete the Fasting blood sugar (fbs) since it is independent from the class label (target) which means that fasting blood sugar (fbs) doesn't affect our class label (target).


```{r}
dataset <- dataset[, -which(names(dataset) == "fbs")]
head(dataset)
```


```{r}
ncol(dataset)
nrow(dataset)
```
Description: After printing a sample from our new data set we notice that the attribute decrease from 14 to 13 since we remove Fasting blood sugar (fbs) and the rows remain the same.


print the final preprocessed dataset
------------------------------------
```{r}
print(data)
```


we apply chi-square and correlation coefficient to know the attribute that affect our class label in preprocessing ,then started work in classification.
Classification:  is a supervised learning technique where a model is trained on a labeled dataset, meaning the dataset has already been tagged or classified with the correct class labels. 
while we try to increase the accuracy, we try different way one of this way  was add  attribute and  remove attribute and we noticed that the following attribute:slope , doesn’t affect accuracy adding them or removing them doesn’t change anything.


-----------------------
check if the data set is balanced or not:
-----------------------
We have previously shown the imbalance of the class label “target ” ( in Pre-processing). Now, we have to balance it to prevent the model from becoming biased towards the majority and thus produce inaccurate results

```{r}
# Identifying the rows of each status
not.target_indices <- which(dataset$target == 0)
target_indices <- which(dataset$target == 1)

# Number of samples for each status class
num_not.target <- length(not.target_indices)
num_target <- length(target_indices)

# Subsampling to balance the data
if (num_not.target > num_target) {
  # Subsample the "not target" status to match the "target" count
  sampled_not.target_indices <- sample(not.target_indices, num_target)
  balanced_dataset <- rbind(dataset[sampled_not.target_indices, ], dataset[target_indices, ])
} else {
  # Subsample the "target" status to match the "not target" count
  sampled_target_indices <- sample(target_indices, num_not.target)
  balanced_dataset <- rbind(dataset[not.target_indices, ], dataset[sampled_target_indices,])
}
```

Verify Class Label Balance



```{r}
# Count of each class in the original dataset
original_class_counts <- table(dataset$target)

# Count of each class in the balanced dataset
balanced_class_counts <- table(balanced_dataset$target)

# Display the counts
print("Original Dataset Class Counts:")
```

```{r}
print(original_class_counts)
```


```{r}
print("Balanced Dataset Class Counts:")
print(balanced_class_counts)
```
There are 159 rows for target  (majority), and 125 rows for not target  (minority) before balancing. After balancing, there are 125 rows for target  and 125 rows for not target  showing a balnced class label ready for classification.


-------------------
Classification:
-------------------
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(partykit)
library(RWeka)
library(tidymodels)
library(tidyr)
library(MASS)
library(rpart)
library(rpart.plot)
library(caret)
```
## split the data in training and test data (75/25)

```{r}
set.seed(123)
data_split <- initial_split(dataset, prop = 0.75)
train_data <- training(data_split)
test_data <- testing(data_split)
```


## build decision tree for each partition and measure

```{r}
tree_1_ig <- rpart(as.factor(target) ~ ., data = train_data, method = "class", parms = list(split = "information"))

tree_1_gini <- rpart(target ~ ., data = train_data, method = "class", parms = list(split = "gini"))

tree_1_gr <- rpart(target ~ ., data = train_data, method = "class", parms = list(split = "anova"))
```

```{r}
myFormula<- target~ age + sex + cp + trestbps + chol + restecg + thalach + exang + oldpeak + slope + ca + thal
dataset_ctree<-ctree(myFormula, data=train_data)
table(predict(dataset_ctree), train_data$target)
```
the showed martix is based on the training data


```{r}
print(dataset_ctree)
```
```{r}
plot(dataset_ctree,type="simple")
```

##plot the decision trees for each measure(IG)

```{r}
rpart.plot(tree_1_ig, main = "Decision Tree for 75/25 Partition using Information Gain")
```
```{r}
#predict the target on the test set
pred_1_ig <- predict(tree_1_ig, newdata = test_data, type = "class")

pred <- as.factor(pred_1_ig) 
actual <- as.factor(test_data$target)

confusionMatrix(pred, actual)

#create a confusion matrix
table(pred_1_ig, test_data$target)

#calculate the overall accuracy
mean(pred_1_ig ==  test_data$target)

```



## plot the decision trees for each measure(Gini index)

```{r}
rpart.plot(tree_1_gini, main = "Decision Tree for 75/25 Partition using gini index")
```
```{r}
#predict the target on the test set
tree_1_gini <- predict(tree_1_gini, newdata = test_data, type = "class")

#create a confusion matrix
table(tree_1_gini, test_data$target)

pred <- as.factor(tree_1_gini) 
actual <- as.factor(test_data$target)

confusionMatrix(pred, actual)

#calculate the overall accuracy
mean(tree_1_gini ==  test_data$target)
```


## plot the decision trees for each measure(GINI ratio)

```{r}
rpart.plot(tree_1_gr, main = "Decision Tree for 75/25 Partition using gini ratio")
```

```{r}
#predict the target on the test set
tree_1_gr <- predict(tree_1_gr, newdata = test_data, type = "class")

#create a confusion matrix
table(tree_1_gr, test_data$target)

pred <- as.factor(tree_1_gr) 
actual <- as.factor(test_data$target)

confusionMatrix(pred, actual)



#calculate the overall accuracy
mean(tree_1_gr ==  test_data$target)

```

This is a decision tree diagram for a 75/25 partition using information gain. It is a graphical representation of a decision-making process, where each node represents a decision and each branch represents a possible outcome.



## C4.5 algorithm (75/25)

```{r}
library(RWeka)
# load the package
library(RWeka)
# load data
# fit model
tree <- J48(as.factor(target) ~ ., data = train_data)
# summarize the fit
summary(tree)
# make predictions
predictions <- predict(tree, test_data)
# summarize accuracy
table(predictions, test_data$target)
#calculate the overall accuracy
mean(predictions ==  test_data$target)
library(partykit)


```


## Cart algorithm (75/25)

```{r}
# load the package
library(ipred)
# fit model
tree <- Bagging (as.factor(target) ~ ., data = train_data)
summary(tree)
# make predictions
predictions <- predict(tree, test_data)
# summarize accuracy
table(predictions, test_data$target)
#calculate the overall accuracy
mean(predictions ==  test_data$target)

```




## split the data in training and test data (80/20)


```{r}
set.seed(123)
data_split <- initial_split(dataset, prop = 0.80)
train_data <- training(data_split)
test_data <- testing(data_split)
```

## build decision tree for each partition and measure

```{r}
tree_1_ig <- rpart(target ~ ., data = train_data, method = "class", parms = list(split = "information"))

tree_1_gini <- rpart(target ~ ., data = train_data, method = "class", parms = list(split = "gini"))

tree_1_gr <- rpart(target ~ ., data = train_data, method = "class", parms = list(split = "anova"))
```

--------


## plot the decision trees for each measure(IG)

```{r}
rpart.plot(tree_1_ig, main = "Decision Tree for 80/20 Partition using Information Gain")
```

```{r}
#predict the species on the test set
pred_1_ig <- predict(tree_1_ig, newdata = test_data, type = "class")

#create a confusion matrix
table(pred_1_ig, test_data$target)

pred <- as.factor(pred_1_ig) 
actual <- as.factor(test_data$target)

confusionMatrix(pred, actual)

#calculate the overall accuracy
mean(pred_1_ig ==  test_data$target)

```

## plot the decision trees for each measure(GINI index)

```{r}
rpart.plot(tree_1_gini, main = "Decision Tree for 80/20 Partition using gini index")
```

##plot the decision trees for each measure(GINI ratio)

```{r}
rpart.plot(tree_1_gr, main = "Decision Tree for 80/20 Partition using gini ratio")
```

This is a decision tree diagram for a 80/20 partition using information gain. It is a graphical representation of a decision-making process, where each node represents a decision and each branch represents a possible outcome.

The tree contains 6 variables with 6 splits. Splits have occurred on the variables ca, Cp, thal, trestbp, and sex.


## predict the target on the test set


#predict the species on the test set
tree_1_gini <- predict(tree_1_gini, newdata = test_data, type = "class")

#create a confusion matrix
table(tree_1_gini, test_data$target)

pred <- as.factor(tree_1_gini) 
actual <- as.factor(test_data$target)

confusionMatrix(pred, actual)

#calculate the overall accuracy
mean(tree_1_gini ==  test_data$target)

#predict the target on the test set
tree_1_gr <- predict(tree_1_gr, newdata = test_data, type = "class")

#create a confusion matrix
table(tree_1_gr, test_data$target)

pred <- as.factor(tree_1_gr) 
actual <- as.factor(test_data$target)

confusionMatrix(pred, actual)

#calculate the overall accuracy
mean(tree_1_gr ==  test_data$target)

```
the confusion matrix of the IG model showed the accuracy of 81%, th gini ratio showed 81%, while the gini index showed 81% accuracy looking at the accuracy level we can see that all the measures performed at thesame accuracy level.


## C4.5 algorithm (80/20)

```{r}
library(RWeka)
# load the package
library(RWeka)
# load data
# fit model
tree <- J48(as.factor(target) ~ ., data = train_data)
# summarize the fit
summary(tree)
# make predictions
predictions <- predict(tree, test_data)
# summarize accuracy
table(predictions, test_data$target)
#calculate the overall accuracy
mean(predictions ==  test_data$target)
library(partykit)

```


## Cart algorithm (80/20)

```{r}
# load the package
library(ipred)
# fit model
tree <- Bagging (as.factor(target) ~ ., data = train_data)
summary(tree)
# make predictions
predictions <- predict(tree, test_data)
# summarize accuracy
table(predictions, test_data$target)
#calculate the overall accuracy
mean(predictions ==  test_data$target)

```




## split the data in training and test data (60/40)


```{r}
set.seed(123)
data_split <- initial_split(dataset, prop = 0.60)
train_data <- training(data_split)
test_data <- testing(data_split)
```


## build decision tree for each partition and measure

```{r}
tree_1_ig <- rpart(target ~ ., data = train_data, method = "class", parms = list(split = "information"))

tree_1_gini <- rpart(target ~ ., data = train_data, method = "class", parms = list(split = "gini"))

tree_1_gr <- rpart(target ~ ., data = train_data, method = "class", parms = list(split = "anova"))
```


##plot the decision trees for each measure(IG)

```{r}
rpart.plot(tree_1_ig, main = "Decision Tree for 60/40 Partition using Information Gain")
```



## plot the decision trees for each measure(GINI index)

```{r}
rpart.plot(tree_1_gini, main = "Decision Tree for 60/40 Partition using gini index")
```

## plot the decision trees for each measure(GINI ratio)

```{r}
rpart.plot(tree_1_gr, main = "Decision Tree for 60/40 Partition using gini ratio")
```


This is a decision tree diagram for a 60/40 partition using information gain. It is a graphical representation of a decision-making process, where each node represents a decision and each branch represents a possible outcome.

The tree contains 6 variables with 6 splits. Splits have occurred on the variables ca, Cp, thal, trestbp, and sex.

## predict the model on the test set

```{r}

#predict the target on the test set
pred_1_ig <- predict(tree_1_ig, newdata = test_data, type = "class")

#create a confusion matrix
table(pred_1_ig, test_data$target)

pred <- as.factor(pred_1_ig) 
actual <- as.factor(test_data$target)

confusionMatrix(pred, actual)

#calculate the overall accuracy
mean(pred_1_ig ==  test_data$target)


#predict the target on the test set
tree_1_gini <- predict(tree_1_gini, newdata = test_data, type = "class")

#create a confusion matrix
table(tree_1_gini, test_data$target)

pred <- as.factor(tree_1_gini) 
actual <- as.factor(test_data$target)

confusionMatrix(pred, actual)

#calculate the overall accuracy
mean(tree_1_gini ==  test_data$target)

#predict the target on the test set
tree_1_gr <- predict(tree_1_ig, newdata = test_data, type = "class")

#create a confusion matrix
table(tree_1_gr, test_data$target)
pred <- as.factor(tree_1_gr) 
actual <- as.factor(test_data$target)
confusionMatrix(pred, actual)
#calculate the overall accuracy
mean(tree_1_gr ==  test_data$target)


```
the confusion matrix of the IG model showed the accuracy of 77%, th gini ration showed 76%, while the gini index showed 77% accuracy looking at the accuracy level we can that the IG model and gini index model measure performed better at classification.


## C4.5 algorithm (60/40)

```{r}
library(RWeka)
# load the package
library(RWeka)
# load data
# fit model
tree <- J48(as.factor(target) ~ ., data = train_data)
# summarize the fit
summary(tree)
# make predictions
predictions <- predict(tree, test_data)
# summarize accuracy
table(predictions, test_data$target)
#calculate the overall accuracy
mean(predictions ==  test_data$target)
library(partykit)
plot(tree)

```


## Cart algorithm (60/40)

```{r}
# load the package
library(ipred)
# fit model
tree <- Bagging (as.factor(target) ~ ., data = train_data)
summary(tree)
# make predictions
predictions <- predict(tree, test_data)
# summarize accuracy
table(predictions, test_data$target)
#calculate the overall accuracy
mean(predictions ==  test_data$target)

```


------------------
discusion:
------------------
from the study we observe that the split 60/40 performed better because the misclassification error is lower and the accuracy value is higer than others.
The decision tree It is a graphical representation of a decision-making process, where each node represents a decision and each branch represents a possible outcome.
The tree contains 6 variables with 6 splits. Splits have occurred on the variables ca, Cp, thal, trestbp, and sex. this are the most important best features.
the c4.5 algorithm gives a high better accuracy, by looking at the confusion matrix and calculating the accuracy of the classification.
---------------------
Findings 
---------------------
                      |75% training set and 25% testing set|80% training set and 20% testing set|60% training set and 40% testing set|
                      ----------------------------------------------------------------------------------------------------------------
                      | IG       | IG ratio   | Gini index | IG       | IG ratio   | Gini index | IG       | IG ratio   | Gini index |
                         
Accuracy              
--------------
precision
--------------
sensitivity
--------------
specificity









